{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mLogfire\u001b[0m project URL: \u001b]8;id=386498;https://logfire.pydantic.dev/gaohe2z/hhl-openai-agents-sdk\u001b\\\u001b[4;36mhttps://logfire.pydantic.dev/gaohe2z/hhl-openai-agents-sdk\u001b[0m\u001b]8;;\u001b\\\n"
     ]
    }
   ],
   "source": [
    "from hhl_openai_agents_sdk.models import get_gemini\n",
    "from dotenv import load_dotenv\n",
    "import nest_asyncio\n",
    "import logfire\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# make sure we can run async code in jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# instrument logfire\n",
    "logfire.configure(token=os.getenv(\"LOGFIRE_TOKEN\"))\n",
    "logfire.instrument_openai()\n",
    "\n",
    "# initialize the model\n",
    "model = get_gemini()\n",
    "# model = get_openai()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent, handoff\n",
    "from agents.extensions import handoff_filters\n",
    "from agents.extensions.handoff_prompt import RECOMMENDED_PROMPT_PREFIX\n",
    "\n",
    "\n",
    "faq_agent = Agent(name=\"FAQ agent\", model=model)\n",
    "\n",
    "faq_agent_handoff = handoff(\n",
    "    agent=faq_agent,\n",
    "    on_handoff=lambda x: print(\"FAQ Handoff called\"),\n",
    "    input_filter=handoff_filters.remove_all_tools,\n",
    ")\n",
    "\n",
    "sales_agent = Agent(name=\"Sales agent\", model=model, instructions=\"\"\"\n",
    "You are a helpful sales agent. You help customers with their product inquiries. Based on the conversation history you\n",
    "should be able to answer the customer questions without having to ask them the questions customer already provided to you.\n",
    "\"\"\")\n",
    "\n",
    "sales_agent_handoff = handoff(\n",
    "    agent=sales_agent,\n",
    "    on_handoff=lambda x: print(\"Sales Handoff called\"),\n",
    ")\n",
    "\n",
    "starter = Agent(\n",
    "    name=\"Customer Agent\",\n",
    "    model=model,\n",
    "    instructions=\"You are a helpful customer agent. You help customers navigate their request to the right agent. \"\n",
    "    \"If customer is asking a question, navigate them to the FAQ agent. \"\n",
    "    \"If customer is looking for a product, navigate them to the sales agent.\",\n",
    "    handoff_description=RECOMMENDED_PROMPT_PREFIX,\n",
    "    handoffs=[faq_agent_handoff, sales_agent_handoff],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Runner\n",
    "async def main():\n",
    "    input_text = \"I want to buy a new phone, my budget is $1000.\"\n",
    "    result = await Runner.run(starter, input_text)\n",
    "    print(result.final_output)\n",
    "    input_text = \"How much does the iPhone 15 cost?\"\n",
    "    inputs = result.to_input_list() + [{\"role\": \"user\", \"content\": input_text}]\n",
    "    result = await Runner.run(starter, inputs)\n",
    "    print(result.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:19:38.433 Chat Completion with 'gemini-2.0-flash' [LLM]\n",
      "Sales Handoff called\n",
      "22:19:38.938 Chat Completion with 'gemini-2.0-flash' [LLM]\n",
      "Hi there! I'm happy to help you find a new phone. To give you the best recommendations, could you tell me a bit about what you're looking for? For example:\n",
      "\n",
      "*   **What's your budget?**\n",
      "*   **What features are most important to you?** (e.g., camera quality, battery life, screen size, specific apps)\n",
      "*   **Do you have a preferred brand or operating system?** (e.g., Apple, Android)\n",
      "\n",
      "Once I have a better understanding of your needs, I can suggest some phones that would be a good fit.\n",
      "\n"
     ]
    },
    {
     "ename": "UserError",
     "evalue": "Unhandled item type or structure: how much does iPhone X cost?",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUserError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m main()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(result.final_output)\n\u001b[32m      6\u001b[39m inputs = result.to_input_list() + [\u001b[33m\"\u001b[39m\u001b[33mhow much does iPhone X cost?\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m Runner.run(starter, inputs)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(result.final_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/github.com/ghengao/hhl-openai-agents-sdk/.venv/lib/python3.12/site-packages/agents/run.py:210\u001b[39m, in \u001b[36mRunner.run\u001b[39m\u001b[34m(cls, starting_agent, input, context, max_turns, hooks, run_config)\u001b[39m\n\u001b[32m    205\u001b[39m logger.debug(\n\u001b[32m    206\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRunning agent \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_agent.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (turn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_turn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    207\u001b[39m )\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m current_turn == \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m     input_guardrail_results, turn_result = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    211\u001b[39m         \u001b[38;5;28mcls\u001b[39m._run_input_guardrails(\n\u001b[32m    212\u001b[39m             starting_agent,\n\u001b[32m    213\u001b[39m             starting_agent.input_guardrails\n\u001b[32m    214\u001b[39m             + (run_config.input_guardrails \u001b[38;5;129;01mor\u001b[39;00m []),\n\u001b[32m    215\u001b[39m             copy.deepcopy(\u001b[38;5;28minput\u001b[39m),\n\u001b[32m    216\u001b[39m             context_wrapper,\n\u001b[32m    217\u001b[39m         ),\n\u001b[32m    218\u001b[39m         \u001b[38;5;28mcls\u001b[39m._run_single_turn(\n\u001b[32m    219\u001b[39m             agent=current_agent,\n\u001b[32m    220\u001b[39m             original_input=original_input,\n\u001b[32m    221\u001b[39m             generated_items=generated_items,\n\u001b[32m    222\u001b[39m             hooks=hooks,\n\u001b[32m    223\u001b[39m             context_wrapper=context_wrapper,\n\u001b[32m    224\u001b[39m             run_config=run_config,\n\u001b[32m    225\u001b[39m             should_run_agent_start_hooks=should_run_agent_start_hooks,\n\u001b[32m    226\u001b[39m         ),\n\u001b[32m    227\u001b[39m     )\n\u001b[32m    228\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    229\u001b[39m     turn_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._run_single_turn(\n\u001b[32m    230\u001b[39m         agent=current_agent,\n\u001b[32m    231\u001b[39m         original_input=original_input,\n\u001b[32m   (...)\u001b[39m\u001b[32m    236\u001b[39m         should_run_agent_start_hooks=should_run_agent_start_hooks,\n\u001b[32m    237\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/tasks.py:385\u001b[39m, in \u001b[36mTask.__wakeup\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[32m    384\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m         \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    386\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    387\u001b[39m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[32m    388\u001b[39m         \u001b[38;5;28mself\u001b[39m.__step(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/tasks.py:314\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    311\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    312\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    313\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    316\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/github.com/ghengao/hhl-openai-agents-sdk/.venv/lib/python3.12/site-packages/agents/run.py:719\u001b[39m, in \u001b[36mRunner._run_single_turn\u001b[39m\u001b[34m(cls, agent, original_input, generated_items, hooks, context_wrapper, run_config, should_run_agent_start_hooks)\u001b[39m\n\u001b[32m    716\u001b[39m \u001b[38;5;28minput\u001b[39m = ItemHelpers.input_to_new_input_list(original_input)\n\u001b[32m    717\u001b[39m \u001b[38;5;28minput\u001b[39m.extend([generated_item.to_input_item() \u001b[38;5;28;01mfor\u001b[39;00m generated_item \u001b[38;5;129;01min\u001b[39;00m generated_items])\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m new_response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_new_response(\n\u001b[32m    720\u001b[39m     agent,\n\u001b[32m    721\u001b[39m     system_prompt,\n\u001b[32m    722\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    723\u001b[39m     output_schema,\n\u001b[32m    724\u001b[39m     handoffs,\n\u001b[32m    725\u001b[39m     context_wrapper,\n\u001b[32m    726\u001b[39m     run_config,\n\u001b[32m    727\u001b[39m )\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_single_step_result_from_response(\n\u001b[32m    730\u001b[39m     agent=agent,\n\u001b[32m    731\u001b[39m     original_input=original_input,\n\u001b[32m   (...)\u001b[39m\u001b[32m    738\u001b[39m     run_config=run_config,\n\u001b[32m    739\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/github.com/ghengao/hhl-openai-agents-sdk/.venv/lib/python3.12/site-packages/agents/run.py:862\u001b[39m, in \u001b[36mRunner._get_new_response\u001b[39m\u001b[34m(cls, agent, system_prompt, input, output_schema, handoffs, context_wrapper, run_config)\u001b[39m\n\u001b[32m    860\u001b[39m model = \u001b[38;5;28mcls\u001b[39m._get_model(agent, run_config)\n\u001b[32m    861\u001b[39m model_settings = agent.model_settings.resolve(run_config.model_settings)\n\u001b[32m--> \u001b[39m\u001b[32m862\u001b[39m new_response = \u001b[38;5;28;01mawait\u001b[39;00m model.get_response(\n\u001b[32m    863\u001b[39m     system_instructions=system_prompt,\n\u001b[32m    864\u001b[39m     \u001b[38;5;28minput\u001b[39m=\u001b[38;5;28minput\u001b[39m,\n\u001b[32m    865\u001b[39m     model_settings=model_settings,\n\u001b[32m    866\u001b[39m     tools=agent.tools,\n\u001b[32m    867\u001b[39m     output_schema=output_schema,\n\u001b[32m    868\u001b[39m     handoffs=handoffs,\n\u001b[32m    869\u001b[39m     tracing=get_model_tracing_impl(\n\u001b[32m    870\u001b[39m         run_config.tracing_disabled, run_config.trace_include_sensitive_data\n\u001b[32m    871\u001b[39m     ),\n\u001b[32m    872\u001b[39m )\n\u001b[32m    874\u001b[39m context_wrapper.usage.add(new_response.usage)\n\u001b[32m    876\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m new_response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/github.com/ghengao/hhl-openai-agents-sdk/.venv/lib/python3.12/site-packages/agents/models/openai_chatcompletions.py:118\u001b[39m, in \u001b[36mOpenAIChatCompletionsModel.get_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, tracing)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_response\u001b[39m(\n\u001b[32m    103\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    104\u001b[39m     system_instructions: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    110\u001b[39m     tracing: ModelTracing,\n\u001b[32m    111\u001b[39m ) -> ModelResponse:\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m generation_span(\n\u001b[32m    113\u001b[39m         model=\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.model),\n\u001b[32m    114\u001b[39m         model_config=dataclasses.asdict(model_settings)\n\u001b[32m    115\u001b[39m         | {\u001b[33m\"\u001b[39m\u001b[33mbase_url\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m._client.base_url)},\n\u001b[32m    116\u001b[39m         disabled=tracing.is_disabled(),\n\u001b[32m    117\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m span_generation:\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m         response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fetch_response(\n\u001b[32m    119\u001b[39m             system_instructions,\n\u001b[32m    120\u001b[39m             \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    121\u001b[39m             model_settings,\n\u001b[32m    122\u001b[39m             tools,\n\u001b[32m    123\u001b[39m             output_schema,\n\u001b[32m    124\u001b[39m             handoffs,\n\u001b[32m    125\u001b[39m             span_generation,\n\u001b[32m    126\u001b[39m             tracing,\n\u001b[32m    127\u001b[39m             stream=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    128\u001b[39m         )\n\u001b[32m    130\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m _debug.DONT_LOG_MODEL_DATA:\n\u001b[32m    131\u001b[39m             logger.debug(\u001b[33m\"\u001b[39m\u001b[33mReceived model response\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/github.com/ghengao/hhl-openai-agents-sdk/.venv/lib/python3.12/site-packages/agents/models/openai_chatcompletions.py:481\u001b[39m, in \u001b[36mOpenAIChatCompletionsModel._fetch_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, span, tracing, stream)\u001b[39m\n\u001b[32m    469\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_fetch_response\u001b[39m(\n\u001b[32m    470\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    471\u001b[39m     system_instructions: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    479\u001b[39m     stream: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    480\u001b[39m ) -> ChatCompletion | \u001b[38;5;28mtuple\u001b[39m[Response, AsyncStream[ChatCompletionChunk]]:\n\u001b[32m--> \u001b[39m\u001b[32m481\u001b[39m     converted_messages = \u001b[43m_Converter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems_to_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    483\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m system_instructions:\n\u001b[32m    484\u001b[39m         converted_messages.insert(\n\u001b[32m    485\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m    486\u001b[39m             {\n\u001b[32m   (...)\u001b[39m\u001b[32m    489\u001b[39m             },\n\u001b[32m    490\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/github.com/ghengao/hhl-openai-agents-sdk/.venv/lib/python3.12/site-packages/agents/models/openai_chatcompletions.py:945\u001b[39m, in \u001b[36m_Converter.items_to_messages\u001b[39m\u001b[34m(cls, items)\u001b[39m\n\u001b[32m    939\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m UserError(\n\u001b[32m    940\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEncountered an item_reference, which is not supported: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem_ref\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    941\u001b[39m         )\n\u001b[32m    943\u001b[39m     \u001b[38;5;66;03m# 7) If we haven't recognized it => fail or ignore\u001b[39;00m\n\u001b[32m    944\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m945\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m UserError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnhandled item type or structure: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    947\u001b[39m flush_assistant_message()\n\u001b[32m    948\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[31mUserError\u001b[39m: Unhandled item type or structure: how much does iPhone X cost?"
     ]
    }
   ],
   "source": [
    "await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
